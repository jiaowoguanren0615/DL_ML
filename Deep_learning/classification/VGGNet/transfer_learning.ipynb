{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3affcee2-941b-41ef-b58e-b836f0e087c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os, sys, json, cv2, random, torchvision\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from numpy import interp\n",
    "import warnings\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torchinfo import summary\n",
    "from sklearn.metrics import auc, f1_score, roc_curve, classification_report, confusion_matrix\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from itertools import cycle\n",
    "\n",
    "\n",
    "from config import device, epochs, root, batch_size, lr, weight_decay, save_path, data_transform, resume, best_val_accuracy\n",
    "from my_dataset import MyDataset\n",
    "from utils import Plot_ROC, train_step, val_step, read_split_data\n",
    "from VGGPredict import predict_single_image, predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dd36f0-eb8f-4f7d-ae76-d8cba83a1306",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_path, train_image_label, val_image_path, val_image_label, class_indices = read_split_data(root)\n",
    "\n",
    "train_dataset = MyDataset(train_image_path, train_image_label, data_transform['train'])\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True,\n",
    "                          collate_fn=train_dataset.collate_fn)\n",
    "\n",
    "valid_dataset = MyDataset(val_image_path, val_image_label, data_transform['valid'])\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True,\n",
    "                          collate_fn=valid_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aaab37ce-b6f2-4faf-80fe-d28ac1fc7556",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16 = torchvision.models.vgg16(weights=torchvision.models.VGG16_Weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27fcced7-ebb0-4daf-bd6e-a16e76b32213",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16.classifier[6].out_features = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e47c833-caeb-451d-8f14-bf6b757f147e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=5, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b38d072-b6c1-429f-a0c1-76db70627cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = vgg16\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a516b5c7-6900-4d6e-a720-f6e5c27ef5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar = torch.cuda.amp.GradScaler() if torch.cuda.is_bf16_supported() else None\n",
    "\n",
    "best_val_accuracy = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # train\n",
    "    train_loss, train_accuracy = train_step(net, optimizer, train_loader, device, epoch, scalar)\n",
    "    # valid\n",
    "    val_loss, val_accuracy = val_step(net, valid_loader, device, epoch)\n",
    "\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        save_parameters = {\n",
    "            'model': net.state_dict(),\n",
    "            'best_accuracy': val_accuracy,\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'lr_scheduler': lr_scheduler.state_dict()\n",
    "        }\n",
    "\n",
    "        torch.save(save_parameters, save_path)\n",
    "\n",
    "print('Now we predict an image!!!')\n",
    "predict_single_image()\n",
    "print('\\n')\n",
    "f1score = predictor(valid_loader)\n",
    "Plot_ROC(net, valid_loader, save_path, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
